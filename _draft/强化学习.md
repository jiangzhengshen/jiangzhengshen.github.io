David Silver Course:
https://github.com/dalmia/David-Silver-Reinforcement-learning
https://github.com/cryer/D.Silver_RL_Course
https://github.com/ShangtongZhang/reinforcement-learning-an-introduction

https://git.code.oa.com/gerrysun/alpha-five/tree/master
https://git.code.oa.com/aaronyongli/GoTetris
https://github.com/yenchenlin/DeepLearningFlappyBird

强化学习：五子棋（gym-gomoku/-Reinforcement-Learning-five-in-a-row-）、poker-learn、https://www.zhihu.com/question/333671830

建议的学习顺序：
- https://www.reddit.com/r/reinforcementlearning/comments/ci1bvy/opinions_on_free_resources_to_learn_deep/

强化学习解决背包问题：
- https://zhuanlan.zhihu.com/p/74967878
- https://www.zhihu.com/question/43610101

强化学习：
- bandit
- 博弈
  - 象棋游戏
  - AlphaGo
  - DeepMind（Atari）
- 自动控制（Autonomous Helicopter Aerobatics through Apprenticeship Learning）
- 推荐算法
- 量化交易 & 投资
- 超参数优化、利用DRL来设计神经网络

Reinforcement Learning (RL) 针对的是 learning with feedback 或者说 sequential decision-making 的问题。与之对应的 Supervised Learning, Unsupervised Learning 针对的都是静态的问题。所以比如说，机器人学习只能用 RL，或者说它本身就是个 RL 的问题。当然，在 RL 的框架下可以使用别的学习方法。

行动者-评论家（Actor-Critic）模型
连续时间决策（自动驾驶）
DeepMind的系列文章

挖掘属性的新鲜度问题
强化学习badcase分析


特点
- 序列决策，前面的动作会影响后面动作的奖赏。例如，股票交易时，前面的一个错误决策会导致本金的损失，进而导致后续的收益。

输入输出
- 输入：当前所处的状态$x$
- 输出：动作$a$

主要方法
- 第一种方法：直接学习函数$a=f(x)$。称为策略梯度法，例如AlphaGo离线部分的第二步。注意：训练时，不是通过$a$的真值来训练的，而是通过$a$得到的奖赏来训练的。
- 第二种方法：学习一个$Q$函数，来估计$Q(x,a)$，然后间接的根据$x$输出$a$。称为$Q$学习，例如AlphaGo离线部分的第三步。

在策略（on-policy）和离策略（off-policy）
- 分享中的强化学习主要是off-policy，特点是训练数据是线下获得的。例如AlphaGo的策略网络，一开始使用人类棋谱训练可视为off-policy的，后来的自我对弈则是on-policy。
- on-policy的特点是边执行边学习，犯了错误马上修正。这种方法的效果通常不如off-policy，主要因为为了保证线上的效果，不能对样本进行充分的探索。而且，样本都是通过尝试得来的，因此训练样本比较有限。
- 两种策略的结合会比较有优势

可能有用的模型（需要进一步调研）
- 行动者-评论家模型
- 连续时间决策（无限的决策序列，而不是下棋这种有限步的决策）
- on-policy

潜在业务结合
- ETA分为若干link，每个link的ETA作为序列决策
- ETA请求是连续的一串请求，视为序列决策，避免跳频繁跳变
- 路况的时空平滑，也是一个序列决策
- 特征有时效，例如hts挖掘时所使用的数据，越旧越不好
- **取代badcase分析的工作，在线学习**

资源
- 深度强化学习综述

---
### 设计
- 状态：

$$ s = (V_{0}, V_{hts}, T_{sum}, L_{left}) $$

- 动作：当前link的ETA

- Q值迭代方程

$$ Q(s, a) = \sum_{s'} p(s'|s,a) \cdot \underset{a'}{\mathrm{max}}\{Q(s',a')\} $$

- 末尾link的Q值：$1000$或$1000/MAPE$

- 任务
  - 统计$p(s'|s,a)$
  - 训练Q网络

- 问题：$p(a|s)$没有用到

---
### 策略梯度求解
类似于股票交易，不使用Q学习

---
### 人造badcase反馈，强化学习
