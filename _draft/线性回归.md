最大似然估计 & KL散度
- 极大似然等价于极小化empirical distribution和你用来拟合的分布的KL散度，参考[这里](https://www.zhihu.com/question/32053138/answer/54640028)

最小二乘法（Ordinary Least Square, OLS）是高斯分布、无先验的最大似然估计
OLS的度量是L2 norm distance，而极大似然的度量是Kullback-Leibler divergence
最大似然的每一种分布假设都会对应一种损失函数呢，例如高斯与最小二乘，拉普拉斯与最小一乘，二项分布与最小零乘等等
带有先验分布的最大似然，会导出有正则项的损失函数，参考[这里](https://www.zhihu.com/question/20447622)
高斯分布 - 欧式距离（损失函数）

那么为啥MLE需要设置分布这么麻烦，还有这么多应用，因为当likelihood设置正确的时候，这个目标函数给出的解最efficient。
那么为啥有这么多人把MLE和OLSE搞混，因为当likelihood用的是gaussian的时候，由于gaussian kernel里有个类似于Euclidean distance的东西，一求log就变成square loss了，导致解和OLSE是一样的。而碰巧刚接触MLE的时候基本都是gaussian假设，这才导致很多人分不清楚。

### 指数分布族
$$ f_{X}(x\mid{\boldsymbol{\theta}})=h(x)\exp {\Big (}{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (x)-A({\boldsymbol {\theta }}){\Big )} $$

或
$$ f_{X}(x\mid {\boldsymbol {\theta }})=h(x)g({\boldsymbol {\theta }})\exp {\Big (}{\boldsymbol {\eta }}({\boldsymbol {\theta }})\cdot \mathbf {T} (x){\Big )} $$

$\boldsymbol{\eta}$为分布的自然参数
$A({\boldsymbol {\theta }})$叫做累积量母函数（又称log partition function）
$\exp(-A({\boldsymbol {\theta }}))$这个量是分布$f_{X}(x\mid{\boldsymbol{\theta}})$的归一化常数，用来确保分布$f_{X}(x\mid{\boldsymbol{\theta}})$对$x$的积分为1
$\mathbf {T} (x)$是**充分统计量**（sufficient statistic），特殊的，$\mathbf {T} (x)=x$时，称为[自然指数族](https://en.wikipedia.org/wiki/Natural_exponential_family)
$T$，$A$和$h$定义了这样一个以$\eta$为参数的分布族。对于不同的$\eta$，我们可以得到指数分布族中不同的分布
当$\forall i: \eta _{i}({\boldsymbol {\theta }})=\theta _{i}$时，称为正则形式(canonical form)

N阶矩等于$A({\boldsymbol {\theta }})$的N阶导数：
$$ \mathrm{E}(T_{j})=\frac{\partial A(\eta)}{\partial \eta_{j}} $$

$$ \mathrm{cov} \left(T_{i},T_{j}\right)={\frac {\partial^{2}A(\eta )}{\partial \eta _{i}\,\partial \eta _{j}}} $$

指数族模型的似然函数（此处假定所有样本具有相同的$\theta$）：
$$ p(D|\boldsymbol{\theta})=\left[\prod^N_{i=1}h(x_i)\right] g(\boldsymbol{\theta})^N \exp\left( \boldsymbol {\eta }({\boldsymbol {\theta }})\cdot \sum^N_{i=1}\mathbf {T}(x_i)\right) $$

$$ p(D|\boldsymbol{\theta})=\left[\prod^N_{i=1}h(x_i)\right] \exp\left( \boldsymbol {\eta }({\boldsymbol {\theta }})\cdot \sum^N_{i=1}\mathbf {T}(x_i) - NA({\boldsymbol{\theta}})\right) $$

充分统计量(sufficient statistics)为
$$ \mathbf{T}(D)=\left[\sum^N_{i=1}T_1(x_i),...,\sum^N_{i=1}T_k(x_i) \right] $$

对伯努利模型为$T=[\sum_i I(x_i=1)]$，单变量高斯模型则有$T=[\sum_i x_i,\sum_i x_i^2]$

#### 正则形式的对数似然函数
即$\forall i: \eta _{i}({\boldsymbol {\theta }})=\theta _{i}$

给定了$N$个独立同分布的数据点，对数似然函数(log-likelihood）中，与$\theta$有关的部分为：
$$ \log p(D|{\boldsymbol{\theta}}) ={\boldsymbol{\theta}}^T\mathbf{T}(D)-NA({\boldsymbol{\theta}}) $$

$-A(\theta)$在$\theta$上是凹的，而$\theta^T\mathbf{T}(D)$在$\theta$上是线性的，因此对数似然函数是凹的，所以这就会有唯一的一个全局最大值。

求导，得
$$ \nabla_\theta\log p(D|\theta)=\mathbf{T}(D)-N\mathrm{E}\left[\mathbf{T}(x)\right] $$

**此式可以用于梯度下降**。

导数为0，即充分统计量的经验均值必须等于模型的理论期望充分统计量，也就是$\hat \theta$必须满足下面的条件:
$$ \mathrm{E}[T_k(x)] = \frac{1}{N}\sum^N_{i=1}T_k(x_i), \ \ \forall k $$

这称为矩捕获（moment matching）。

比如在伯努利分布中,就有$T(x)=I(x=1)$，所以最大似然估计(MLE)就满足：
$$ \mathrm{E}[T(x)]=p(x=1)=\hat \mu =\frac{1}{N}\sum^N_{i=1}I(x_i=1) $$

自然参数的牛顿法等价于自然梯度。即自然参数用普通牛顿法，或任意参数用自然梯度

### 特例：正态分布
$$ f(x;\mu ,\sigma )={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}} $$

$$ \begin{aligned}
{\boldsymbol {\eta }}&=\left({\frac {\mu }{\sigma ^{2}}},-{\frac {1}{2\sigma ^{2}}}\right)^{\rm {T}}\\
h(x)&={\frac {1}{\sqrt {2\pi }}}\\
T(x)&=\left(x,x^{2}\right)^{\rm {T}}\\
A({\boldsymbol {\eta }})&={\frac {\mu ^{2}}{2\sigma ^{2}}}+\log |\sigma |=-{\frac {\eta _{1}^{2}}{4\eta _{2}}}+{\frac {1}{2}}\log \left|{\frac {1}{2\eta _{2}}}\right|
\end{aligned} $$

给定了$N$个独立同分布的数据点，对数似然函数(log-likelihood）中，与$\theta$有关的部分为：
$$ \begin{aligned}
\log p(D|{\boldsymbol{\theta}}) &= \sum^N_{i=1}\left[\boldsymbol {\eta }({\boldsymbol {\theta_i}})\cdot \mathbf {T}(x_i) - A({\boldsymbol{\theta_i}}) \right] \\
&= \sum^N_{i=1}\left[ \frac{\mu}{\sigma^{2}}x_i - \frac {1}{2\sigma ^{2}}x^2_i - \left( {\frac{\mu^{2}}{2\sigma^{2}}}+\log|\sigma| \right) \right] \\
&= \sum^N_{i=1}\left[ \eta_1 x_i + \eta_2 x^2_i - \left( -{\frac {\eta _{1}^{2}}{4\eta _{2}}}+{\frac {1}{2}}\log \left|{\frac {1}{2\eta _{2}}}\right| \right)\right]
\end{aligned} $$

在$(\mu, \sigma^2)$参数下，令$Var=\sigma^2$，损失函数为
$$ \frac{(y-\mu)^2}{Var} + \log{Var} $$

**广义线性模型一般用牛顿法求解，对于梯度下降，自然参数可能不是最好的选择**

### 广义线性模型
1. $y|\boldsymbol{x};\boldsymbol{\beta} \sim ExpFamily(\boldsymbol{\eta})$
2. $\boldsymbol{\eta} = \boldsymbol{x}\boldsymbol{\beta}$，即通过特征拟合与期望有关的自然参数
3. $\mathrm{E}\left[y|\boldsymbol{x}\right]=\mu=g^{-1}(\boldsymbol{\eta})$

### 附录：不确定度估计的推导
设$y$表示ATA，$\mu$表示其ETA估计，$\sigma$表示预估的标准差，$Var$表示方差。由于$\mu$和$Var$是由神经网络预测出来的，因此是网络参数的函数，因此也可以写成$\mu(\theta)$和$Var(\theta)$，其中$\theta$表示网络参数。假设：
$$ p(y;\mu ,\sigma )={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-{\frac {(y-\mu )^{2}}{2\sigma ^{2}}}} $$

那么，给定了$N$个独立同分布的数据点，对数似然函数为：
$$ \begin{aligned}
\log p(D|\mu_1, \mu_2, ..., \sigma_1^2, \sigma_2^2, ...) &= \log \prod_{i=1}^N p(y_i; \mu_i, \sigma_i^2) \\
&= \sum_{i=1}^N \log\left[{\frac {1}{\sqrt {2\pi \sigma_i^{2}}}}e^{-{\frac {(y_i-\mu_i)^{2}}{2\sigma_i^{2}}}}\right] \\
&= \sum_{i=1}^N \left[ -{\frac{(y_i-\mu_i)^{2}}{2\sigma_i^{2}}} - \frac12\log{\sigma_i^2} - \frac12\log{2\pi} \right]
\end{aligned} $$

因此，最大化对数似然函数，等价于最小化下面目标函数：
$$ \argmin_\theta {\sum_{i=1}^N \left[ {\frac{\left[y_i-\mu_i(\theta)\right]^{2}}{Var_i(\theta)}} + \log{Var_i(\theta)} \right] } $$

因此，对于单个样本，其损失函数为：
$$ loss(y;\theta) = {\frac{\left[y-\mu(\theta)\right]^{2}}{Var(\theta)}} + \log{Var(\theta)} $$
