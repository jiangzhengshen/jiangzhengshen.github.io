量化通常是float32转为int8的计算，利用了CPU或GPU在int8计算上的高效性

所以要分成两步：
- 模型量化
- 针对CPU的int8计算优化

### 模型量化方法

### CPU/GPU int8计算优化
http://mk.oa.com/q/255703：里面有优质回答

相关技术：llvm、tvm、NNVM、NEON、XLA、onnx

针对GPU的优化库有：tensorRT、tf-TRT

针对Android端的优化库（前向推理库）比较多，如tflite、pocketflow、gemm、ncnn、mace、mnn、rapidnet

针对Intel CPU（x86）的优化，可以用下面方案
- SSE、AVX这类的SIMD指令手撸各个OP
- MKL-DNN，底层优化库，需要手写推理代码
- OpenVINO，英特尔的优化库，对pb模型进行格式转换，然后服务端修改推理代码，线上不依赖tfserving (https://juejin.im/post/5dd4db045188252ac0471adc)
- TVM，中间表示，可编译到多个平台，包括x86

算子级优化：
- 降低乘法次数：winograd，已包含在OpenVINO、ONNX等库中

技术选择：
- OpenVINO（https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html）
- TVM
- wxg-contrib / HIE

tensorflow模型转换（https://zhuanlan.zhihu.com/p/72562989）
tensorflow模型剪枝 & 稀疏化（https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras；https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-pruning-api-42cac9157a6a）

tfserving性能优化：https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf

训练加速：混合精度训练
